{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGviAwftPZz-",
        "outputId": "d980e957-9be6-4c9e-f0a6-4c0d7d37fe83"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount= True)\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/cycle_gan_h2z\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUC8mnNBOhNA",
        "outputId": "668dc2f6-e35f-4b8d-8625-376440f3f637"
      },
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from gen_model import Generator\n",
        "from dis_model import Discriminator\n",
        "from function import ImageDataset\n",
        "from function import ReplayBuffer\n",
        "from function import LambdaLR\n",
        "\n",
        "\n",
        "lr = 0.0002\n",
        "num_epochs = 200\n",
        "batch_size = 1\n",
        "start_epoch = 0\n",
        "decay_epoch = 110\n",
        "lambda_cycle = 10\n",
        "lambda_iden = 3\n",
        "\n",
        "\n",
        "def initial_weights(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        elif classname.find('BatchNorm2d') != -1:\n",
        "            torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "\n",
        "transforms_trn = [ transforms.Resize(int(256*1.12), Image.BICUBIC), \n",
        "                transforms.RandomCrop(256), \n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)) ]\n",
        "trn_set = ImageDataset(\"./data/grumpifycat/\", transforms_=transforms_trn, mode='train')\n",
        "trn_loader = DataLoader(trn_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "\n",
        "# gen & dis\n",
        "G, F = Generator(num_blocks=9), Generator(num_blocks=9)\n",
        "D_X, D_Y = Discriminator(), Discriminator()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "G.to(device)\n",
        "F.to(device)\n",
        "D_X.to(device)\n",
        "D_Y.to(device)\n",
        "\n",
        "G.load_state_dict(torch.load('output/G.pth'))\n",
        "F.load_state_dict(torch.load('output/F.pth'))\n",
        "D_X.load_state_dict(torch.load('output/D_X.pth'))\n",
        "D_Y.load_state_dict(torch.load('output/D_Y.pth'))\n",
        "\n",
        "# weights\n",
        "\n",
        "#G.apply(initial_weights)\n",
        "#F.apply(initial_weights)\n",
        "#D_X.apply(initial_weights)\n",
        "#D_Y.apply(initial_weights)\n",
        "\n",
        "\n",
        "#loss\n",
        "GAN_loss = nn.MSELoss()\n",
        "cycle_loss = nn.L1Loss()\n",
        "iden_loss = nn.L1Loss()\n",
        "\n",
        "# optim\n",
        "optimizer_G_F = optim.Adam(itertools.chain(G.parameters(), F.parameters()), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D_X = optim.Adam(D_X.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D_Y = optim.Adam(D_Y.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# lr scheduler\n",
        "lr_scheduler_G_F = torch.optim.lr_scheduler.LambdaLR(optimizer_G_F, lr_lambda=LambdaLR(num_epochs, start_epoch, decay_epoch).step)\n",
        "lr_scheduler_D_X = torch.optim.lr_scheduler.LambdaLR(optimizer_D_X, lr_lambda=LambdaLR(num_epochs, start_epoch, decay_epoch).step)\n",
        "lr_scheduler_D_Y = torch.optim.lr_scheduler.LambdaLR(optimizer_D_Y, lr_lambda=LambdaLR(num_epochs, start_epoch, decay_epoch).step)\n",
        "\n",
        "'''\n",
        "def Loss1(input, net, loss, target):\n",
        "    output = net(input)\n",
        "    return loss(output, target)\n",
        "\n",
        "def Loss2(input, net1, net2, loss, target):\n",
        "    output1 = net1(input)\n",
        "    output2 = net2(output1)\n",
        "    return loss(output2, target)\n",
        "'''\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor\n",
        "#Tensor = torch.Tensor\n",
        "fake_Xs, fake_Ys = ReplayBuffer(), ReplayBuffer()\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start = time.time()\n",
        "    Loss_X = 0\n",
        "    Loss_Y = 0\n",
        "    Loss_GF = 0\n",
        "\n",
        "    for idx, data in enumerate(trn_loader):\n",
        "\n",
        "        #X = data['X'].to(device)\n",
        "        #Y = data['Y'].to(device)\n",
        "        X = Variable(Tensor(batch_size, 3, 256, 256).copy_(data['X']))\n",
        "        Y = Variable(Tensor(batch_size, 3, 256, 256).copy_(data['Y']))\n",
        "\n",
        "\n",
        "        # loss of G & F\n",
        "        optimizer_G_F.zero_grad()\n",
        "        for param in G.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in F.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in D_X.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in D_Y.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        G_X, F_Y = G(X), F(Y)\n",
        "        F_X, G_Y = F(X), G(Y)\n",
        "        F_G_X, G_F_Y = F(G_X), G(F_Y)\n",
        "        D_Y_X, D_X_Y = D_Y(G_X), D_X(F_Y)\n",
        "        \n",
        "        Loss_Iden = iden_loss(F_X, X) + iden_loss(G_Y, Y)\n",
        "        Loss_Cycle = cycle_loss(F_G_X, X) + cycle_loss(G_F_Y, Y)\n",
        "        Loss_GAN = GAN_loss(D_Y_X, Tensor(batch_size).fill_(1.0)) + GAN_loss(D_X_Y, Tensor(batch_size).fill_(1.0))\n",
        "        #Loss_Iden = (Loss1(X, F, iden_loss, X)+Loss1(Y, G, iden_loss, X)) * lambda_iden\n",
        "        #Loss_Cycle = (Loss2(X, G, F, cycle_loss, X)+Loss2(Y, F, G, cycle_loss, Y)) * lambda_cycle\n",
        "        #Loss_GAN = Loss2(X, G, D_Y, GAN_loss, Tensor(batch_size).fill_(1.0)) + Loss2(Y, F, D_X, GAN_loss, Tensor(batch_size).fill_(1.0))\n",
        "\n",
        "        Loss_G_F = lambda_iden*Loss_Iden + lambda_cycle*Loss_Cycle + Loss_GAN\n",
        "        Loss_G_F.backward()\n",
        "        for group in optimizer_G_F.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = optimizer_G_F.state[p]\n",
        "                if 'step' in state.keys():\n",
        "                    if ('step' in state and state['step']>=1022):\n",
        "                        state['step'] = 1000\n",
        "        optimizer_G_F.step()\n",
        "\n",
        "\n",
        "        # loss of D_X\n",
        "        for param in G.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in F.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in D_X.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        optimizer_D_X.zero_grad()\n",
        "        Loss_Real = GAN_loss(D_X(X), Tensor(batch_size).fill_(1.0))\n",
        "        #Loss_Real = Loss1(X, D_X, GAN_loss, Tensor(batch_size).fill_(1.0))\n",
        "        fake_X = fake_Xs.add_sample(F(Y), batch_size)\n",
        "        Loss_Fake = GAN_loss(D_X(fake_X.detach()), Tensor(batch_size).fill_(0.0))\n",
        "\n",
        "        Loss_D_X = .5 * (Loss_Real + Loss_Fake)\n",
        "        Loss_D_X.backward()\n",
        "        for group in optimizer_D_X.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = optimizer_D_X.state[p]\n",
        "                if 'step' in state.keys():\n",
        "                    if ('step' in state and state['step']>=1022):\n",
        "                        state['step'] = 1000\n",
        "        optimizer_D_X.step()\n",
        "\n",
        "\n",
        "        # loss of D_Y\n",
        "        for param in D_X.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in D_Y.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        optimizer_D_Y.zero_grad()\n",
        "        Loss_Real = GAN_loss(D_Y(Y), Tensor(batch_size).fill_(1.0))\n",
        "        #Loss_Real = Loss1(Y, D_Y, GAN_loss, Tensor(batch_size).fill_(1.0))\n",
        "        fake_Y = fake_Ys.add_sample(G(X), batch_size)\n",
        "        Loss_Fake = GAN_loss(D_Y(fake_Y.detach()), Tensor(batch_size).fill_(0.0))\n",
        "\n",
        "        Loss_D_Y = .5 * (Loss_Real + Loss_Fake)\n",
        "        Loss_D_Y.backward()\n",
        "        for group in optimizer_D_Y.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = optimizer_D_Y.state[p]\n",
        "                if 'step' in state.keys():\n",
        "                    if ('step' in state and state['step']>=1022):\n",
        "                        state['step'] = 1000\n",
        "        optimizer_D_Y.step()\n",
        "\n",
        "\n",
        "        Loss_GF += Loss_G_F.item()\n",
        "        Loss_X += Loss_D_X.item()\n",
        "        Loss_Y += Loss_D_Y.item()\n",
        "\n",
        "    Loss_GF /= len(trn_loader)\n",
        "    Loss_X /= len(trn_loader)\n",
        "    Loss_Y /= len(trn_loader)\n",
        "\n",
        "    lr_scheduler_G_F.step()\n",
        "    lr_scheduler_D_X.step()\n",
        "    lr_scheduler_D_Y.step()\n",
        "    print('Epoch %d | Cost %.1f sec' % (epoch + 1, time.time() - start))\n",
        "    print('Loss_D_X %.6f | Loss_D_Y %.6f | Loss_G_F %.6f' % (Loss_X, Loss_Y, Loss_GF))\n",
        "\n",
        "    torch.save(G.state_dict(), 'output/G.pth')\n",
        "    torch.save(F.state_dict(), 'output/F.pth')\n",
        "    torch.save(D_X.state_dict(), 'output/D_X.pth')\n",
        "    torch.save(D_Y.state_dict(), 'output/D_Y.pth')\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 | Cost 42.8 sec\n",
            "Loss_D_X 0.140520 | Loss_D_Y 0.109165 | Loss_G_F 5.028938\n",
            "Epoch 2 | Cost 42.9 sec\n",
            "Loss_D_X 0.139224 | Loss_D_Y 0.125295 | Loss_G_F 4.761884\n",
            "Epoch 3 | Cost 42.8 sec\n",
            "Loss_D_X 0.148684 | Loss_D_Y 0.121929 | Loss_G_F 4.763425\n",
            "Epoch 4 | Cost 42.8 sec\n",
            "Loss_D_X 0.133854 | Loss_D_Y 0.122320 | Loss_G_F 4.768203\n",
            "Epoch 5 | Cost 42.9 sec\n",
            "Loss_D_X 0.126009 | Loss_D_Y 0.099578 | Loss_G_F 5.085980\n",
            "Epoch 6 | Cost 42.9 sec\n",
            "Loss_D_X 0.127690 | Loss_D_Y 0.109807 | Loss_G_F 4.904946\n",
            "Epoch 7 | Cost 42.8 sec\n",
            "Loss_D_X 0.128652 | Loss_D_Y 0.128865 | Loss_G_F 4.637910\n",
            "Epoch 8 | Cost 42.8 sec\n",
            "Loss_D_X 0.138731 | Loss_D_Y 0.124494 | Loss_G_F 4.530291\n",
            "Epoch 9 | Cost 42.8 sec\n",
            "Loss_D_X 0.122490 | Loss_D_Y 0.122595 | Loss_G_F 4.643228\n",
            "Epoch 10 | Cost 42.8 sec\n",
            "Loss_D_X 0.113165 | Loss_D_Y 0.116107 | Loss_G_F 4.701697\n",
            "Epoch 11 | Cost 43.0 sec\n",
            "Loss_D_X 0.118752 | Loss_D_Y 0.113429 | Loss_G_F 4.712018\n",
            "Epoch 12 | Cost 42.9 sec\n",
            "Loss_D_X 0.121431 | Loss_D_Y 0.120519 | Loss_G_F 4.565895\n",
            "Epoch 13 | Cost 42.8 sec\n",
            "Loss_D_X 0.134574 | Loss_D_Y 0.121727 | Loss_G_F 4.414649\n",
            "Epoch 14 | Cost 42.7 sec\n",
            "Loss_D_X 0.110540 | Loss_D_Y 0.127610 | Loss_G_F 4.496425\n",
            "Epoch 15 | Cost 42.8 sec\n",
            "Loss_D_X 0.123865 | Loss_D_Y 0.123693 | Loss_G_F 4.514449\n",
            "Epoch 16 | Cost 42.8 sec\n",
            "Loss_D_X 0.107129 | Loss_D_Y 0.117249 | Loss_G_F 4.384761\n",
            "Epoch 17 | Cost 42.8 sec\n",
            "Loss_D_X 0.116633 | Loss_D_Y 0.119513 | Loss_G_F 4.588795\n",
            "Epoch 18 | Cost 42.7 sec\n",
            "Loss_D_X 0.116377 | Loss_D_Y 0.123966 | Loss_G_F 4.370460\n",
            "Epoch 19 | Cost 42.8 sec\n",
            "Loss_D_X 0.112328 | Loss_D_Y 0.134174 | Loss_G_F 4.380389\n",
            "Epoch 20 | Cost 42.9 sec\n",
            "Loss_D_X 0.106637 | Loss_D_Y 0.116737 | Loss_G_F 4.459534\n",
            "Epoch 21 | Cost 42.8 sec\n",
            "Loss_D_X 0.105928 | Loss_D_Y 0.110311 | Loss_G_F 4.380690\n",
            "Epoch 22 | Cost 42.8 sec\n",
            "Loss_D_X 0.110547 | Loss_D_Y 0.127112 | Loss_G_F 4.551509\n",
            "Epoch 23 | Cost 42.8 sec\n",
            "Loss_D_X 0.111838 | Loss_D_Y 0.120484 | Loss_G_F 4.344352\n",
            "Epoch 24 | Cost 42.7 sec\n",
            "Loss_D_X 0.111183 | Loss_D_Y 0.138556 | Loss_G_F 4.508531\n",
            "Epoch 25 | Cost 42.7 sec\n",
            "Loss_D_X 0.097075 | Loss_D_Y 0.117104 | Loss_G_F 4.506415\n",
            "Epoch 26 | Cost 42.7 sec\n",
            "Loss_D_X 0.126757 | Loss_D_Y 0.134136 | Loss_G_F 4.378055\n",
            "Epoch 27 | Cost 42.8 sec\n",
            "Loss_D_X 0.112964 | Loss_D_Y 0.119206 | Loss_G_F 4.415355\n",
            "Epoch 28 | Cost 44.1 sec\n",
            "Loss_D_X 0.112948 | Loss_D_Y 0.131722 | Loss_G_F 4.319770\n",
            "Epoch 29 | Cost 42.9 sec\n",
            "Loss_D_X 0.121757 | Loss_D_Y 0.122248 | Loss_G_F 4.373006\n",
            "Epoch 30 | Cost 42.7 sec\n",
            "Loss_D_X 0.098834 | Loss_D_Y 0.127615 | Loss_G_F 4.331069\n",
            "Epoch 31 | Cost 42.8 sec\n",
            "Loss_D_X 0.099925 | Loss_D_Y 0.116483 | Loss_G_F 4.533736\n",
            "Epoch 32 | Cost 42.8 sec\n",
            "Loss_D_X 0.096681 | Loss_D_Y 0.108232 | Loss_G_F 4.531607\n",
            "Epoch 33 | Cost 42.8 sec\n",
            "Loss_D_X 0.089491 | Loss_D_Y 0.107047 | Loss_G_F 4.516960\n",
            "Epoch 34 | Cost 42.9 sec\n",
            "Loss_D_X 0.104446 | Loss_D_Y 0.121420 | Loss_G_F 4.441958\n",
            "Epoch 35 | Cost 42.9 sec\n",
            "Loss_D_X 0.085815 | Loss_D_Y 0.111867 | Loss_G_F 4.587671\n",
            "Epoch 36 | Cost 42.8 sec\n",
            "Loss_D_X 0.104679 | Loss_D_Y 0.112497 | Loss_G_F 4.416367\n",
            "Epoch 37 | Cost 42.8 sec\n",
            "Loss_D_X 0.093250 | Loss_D_Y 0.107339 | Loss_G_F 4.339673\n",
            "Epoch 38 | Cost 42.8 sec\n",
            "Loss_D_X 0.085383 | Loss_D_Y 0.124420 | Loss_G_F 4.595180\n",
            "Epoch 39 | Cost 42.6 sec\n",
            "Loss_D_X 0.089633 | Loss_D_Y 0.109485 | Loss_G_F 4.442305\n",
            "Epoch 40 | Cost 42.7 sec\n",
            "Loss_D_X 0.103863 | Loss_D_Y 0.121985 | Loss_G_F 4.304897\n",
            "Epoch 41 | Cost 42.8 sec\n",
            "Loss_D_X 0.111099 | Loss_D_Y 0.114135 | Loss_G_F 4.377390\n",
            "Epoch 42 | Cost 42.7 sec\n",
            "Loss_D_X 0.106298 | Loss_D_Y 0.130207 | Loss_G_F 4.253268\n",
            "Epoch 43 | Cost 42.8 sec\n",
            "Loss_D_X 0.091309 | Loss_D_Y 0.100507 | Loss_G_F 4.534996\n",
            "Epoch 44 | Cost 42.7 sec\n",
            "Loss_D_X 0.096634 | Loss_D_Y 0.111409 | Loss_G_F 4.229625\n",
            "Epoch 45 | Cost 42.7 sec\n",
            "Loss_D_X 0.079016 | Loss_D_Y 0.100674 | Loss_G_F 4.572579\n",
            "Epoch 46 | Cost 42.8 sec\n",
            "Loss_D_X 0.085186 | Loss_D_Y 0.117276 | Loss_G_F 4.423077\n",
            "Epoch 47 | Cost 42.8 sec\n",
            "Loss_D_X 0.088755 | Loss_D_Y 0.119287 | Loss_G_F 4.260203\n",
            "Epoch 48 | Cost 42.8 sec\n",
            "Loss_D_X 0.057084 | Loss_D_Y 0.120441 | Loss_G_F 4.494110\n",
            "Epoch 49 | Cost 42.8 sec\n",
            "Loss_D_X 0.075153 | Loss_D_Y 0.129467 | Loss_G_F 4.338338\n",
            "Epoch 50 | Cost 42.7 sec\n",
            "Loss_D_X 0.062832 | Loss_D_Y 0.114525 | Loss_G_F 4.327021\n",
            "Epoch 51 | Cost 43.5 sec\n",
            "Loss_D_X 0.073905 | Loss_D_Y 0.120154 | Loss_G_F 4.279930\n",
            "Epoch 52 | Cost 42.7 sec\n",
            "Loss_D_X 0.084914 | Loss_D_Y 0.119036 | Loss_G_F 4.206080\n",
            "Epoch 53 | Cost 42.8 sec\n",
            "Loss_D_X 0.072193 | Loss_D_Y 0.112006 | Loss_G_F 4.236551\n",
            "Epoch 54 | Cost 42.8 sec\n",
            "Loss_D_X 0.075041 | Loss_D_Y 0.115393 | Loss_G_F 4.245655\n",
            "Epoch 55 | Cost 42.7 sec\n",
            "Loss_D_X 0.074590 | Loss_D_Y 0.118070 | Loss_G_F 4.200880\n",
            "Epoch 56 | Cost 42.8 sec\n",
            "Loss_D_X 0.073977 | Loss_D_Y 0.118867 | Loss_G_F 4.127069\n",
            "Epoch 57 | Cost 42.8 sec\n",
            "Loss_D_X 0.069144 | Loss_D_Y 0.102959 | Loss_G_F 4.202694\n",
            "Epoch 58 | Cost 42.9 sec\n",
            "Loss_D_X 0.080759 | Loss_D_Y 0.114651 | Loss_G_F 4.150678\n",
            "Epoch 59 | Cost 42.7 sec\n",
            "Loss_D_X 0.076255 | Loss_D_Y 0.114699 | Loss_G_F 4.236031\n",
            "Epoch 60 | Cost 42.7 sec\n",
            "Loss_D_X 0.071160 | Loss_D_Y 0.112654 | Loss_G_F 4.184280\n",
            "Epoch 61 | Cost 42.8 sec\n",
            "Loss_D_X 0.076832 | Loss_D_Y 0.099541 | Loss_G_F 4.207751\n",
            "Epoch 62 | Cost 42.7 sec\n",
            "Loss_D_X 0.073169 | Loss_D_Y 0.118472 | Loss_G_F 4.121215\n",
            "Epoch 63 | Cost 42.7 sec\n",
            "Loss_D_X 0.070435 | Loss_D_Y 0.107016 | Loss_G_F 4.234520\n",
            "Epoch 64 | Cost 42.8 sec\n",
            "Loss_D_X 0.061281 | Loss_D_Y 0.114558 | Loss_G_F 4.205871\n",
            "Epoch 65 | Cost 42.7 sec\n",
            "Loss_D_X 0.063667 | Loss_D_Y 0.115658 | Loss_G_F 4.158680\n",
            "Epoch 66 | Cost 42.7 sec\n",
            "Loss_D_X 0.064377 | Loss_D_Y 0.114967 | Loss_G_F 4.078042\n",
            "Epoch 67 | Cost 42.7 sec\n",
            "Loss_D_X 0.069881 | Loss_D_Y 0.121563 | Loss_G_F 4.017438\n",
            "Epoch 68 | Cost 42.7 sec\n",
            "Loss_D_X 0.079080 | Loss_D_Y 0.127666 | Loss_G_F 4.038232\n",
            "Epoch 69 | Cost 42.7 sec\n",
            "Loss_D_X 0.062115 | Loss_D_Y 0.115639 | Loss_G_F 4.046268\n",
            "Epoch 70 | Cost 42.7 sec\n",
            "Loss_D_X 0.049612 | Loss_D_Y 0.117410 | Loss_G_F 4.089166\n",
            "Epoch 71 | Cost 42.8 sec\n",
            "Loss_D_X 0.060673 | Loss_D_Y 0.120350 | Loss_G_F 4.015876\n",
            "Epoch 72 | Cost 42.7 sec\n",
            "Loss_D_X 0.058483 | Loss_D_Y 0.114935 | Loss_G_F 4.001125\n",
            "Epoch 73 | Cost 42.7 sec\n",
            "Loss_D_X 0.058593 | Loss_D_Y 0.114230 | Loss_G_F 4.020079\n",
            "Epoch 74 | Cost 44.0 sec\n",
            "Loss_D_X 0.052010 | Loss_D_Y 0.105500 | Loss_G_F 4.181242\n",
            "Epoch 75 | Cost 42.7 sec\n",
            "Loss_D_X 0.059002 | Loss_D_Y 0.114364 | Loss_G_F 4.055713\n",
            "Epoch 76 | Cost 42.9 sec\n",
            "Loss_D_X 0.049466 | Loss_D_Y 0.110198 | Loss_G_F 3.917722\n",
            "Epoch 77 | Cost 42.9 sec\n",
            "Loss_D_X 0.057370 | Loss_D_Y 0.110746 | Loss_G_F 4.022597\n",
            "Epoch 78 | Cost 42.7 sec\n",
            "Loss_D_X 0.055652 | Loss_D_Y 0.117687 | Loss_G_F 4.015862\n",
            "Epoch 79 | Cost 42.9 sec\n",
            "Loss_D_X 0.051233 | Loss_D_Y 0.110125 | Loss_G_F 3.963942\n",
            "Epoch 80 | Cost 42.8 sec\n",
            "Loss_D_X 0.050024 | Loss_D_Y 0.111497 | Loss_G_F 3.934840\n",
            "Epoch 81 | Cost 42.9 sec\n",
            "Loss_D_X 0.050301 | Loss_D_Y 0.111505 | Loss_G_F 3.975043\n",
            "Epoch 82 | Cost 43.0 sec\n",
            "Loss_D_X 0.042634 | Loss_D_Y 0.102432 | Loss_G_F 3.902002\n",
            "Epoch 83 | Cost 43.0 sec\n",
            "Loss_D_X 0.057305 | Loss_D_Y 0.112495 | Loss_G_F 4.021792\n",
            "Epoch 84 | Cost 43.0 sec\n",
            "Loss_D_X 0.036902 | Loss_D_Y 0.108916 | Loss_G_F 3.926660\n",
            "Epoch 85 | Cost 43.0 sec\n",
            "Loss_D_X 0.039898 | Loss_D_Y 0.118053 | Loss_G_F 3.936112\n",
            "Epoch 86 | Cost 43.2 sec\n",
            "Loss_D_X 0.043307 | Loss_D_Y 0.108624 | Loss_G_F 3.796569\n",
            "Epoch 87 | Cost 43.2 sec\n",
            "Loss_D_X 0.044250 | Loss_D_Y 0.110883 | Loss_G_F 3.893171\n",
            "Epoch 88 | Cost 43.3 sec\n",
            "Loss_D_X 0.040733 | Loss_D_Y 0.114097 | Loss_G_F 3.915439\n",
            "Epoch 89 | Cost 43.2 sec\n",
            "Loss_D_X 0.039204 | Loss_D_Y 0.109429 | Loss_G_F 3.849410\n",
            "Epoch 90 | Cost 43.2 sec\n",
            "Loss_D_X 0.035268 | Loss_D_Y 0.112876 | Loss_G_F 3.855107\n",
            "Epoch 91 | Cost 43.0 sec\n",
            "Loss_D_X 0.040046 | Loss_D_Y 0.105523 | Loss_G_F 3.850517\n",
            "Epoch 92 | Cost 42.9 sec\n",
            "Loss_D_X 0.031585 | Loss_D_Y 0.107179 | Loss_G_F 3.776947\n",
            "Epoch 93 | Cost 42.9 sec\n",
            "Loss_D_X 0.037653 | Loss_D_Y 0.108488 | Loss_G_F 3.863386\n",
            "Epoch 94 | Cost 42.9 sec\n",
            "Loss_D_X 0.038214 | Loss_D_Y 0.110204 | Loss_G_F 3.827903\n",
            "Epoch 95 | Cost 43.0 sec\n",
            "Loss_D_X 0.035416 | Loss_D_Y 0.106307 | Loss_G_F 3.817119\n",
            "Epoch 96 | Cost 43.0 sec\n",
            "Loss_D_X 0.034977 | Loss_D_Y 0.101682 | Loss_G_F 3.868081\n",
            "Epoch 97 | Cost 43.0 sec\n",
            "Loss_D_X 0.031036 | Loss_D_Y 0.106770 | Loss_G_F 3.888031\n",
            "Epoch 98 | Cost 43.0 sec\n",
            "Loss_D_X 0.031369 | Loss_D_Y 0.108232 | Loss_G_F 3.778185\n",
            "Epoch 99 | Cost 42.9 sec\n",
            "Loss_D_X 0.026502 | Loss_D_Y 0.097829 | Loss_G_F 3.724719\n",
            "Epoch 100 | Cost 42.9 sec\n",
            "Loss_D_X 0.026709 | Loss_D_Y 0.109418 | Loss_G_F 3.714754\n",
            "Epoch 101 | Cost 43.0 sec\n",
            "Loss_D_X 0.033247 | Loss_D_Y 0.100362 | Loss_G_F 3.724521\n",
            "Epoch 102 | Cost 42.9 sec\n",
            "Loss_D_X 0.028026 | Loss_D_Y 0.105973 | Loss_G_F 3.759919\n",
            "Epoch 103 | Cost 42.9 sec\n",
            "Loss_D_X 0.032833 | Loss_D_Y 0.105708 | Loss_G_F 3.774098\n",
            "Epoch 104 | Cost 42.9 sec\n",
            "Loss_D_X 0.032440 | Loss_D_Y 0.106772 | Loss_G_F 3.718645\n",
            "Epoch 105 | Cost 42.9 sec\n",
            "Loss_D_X 0.026766 | Loss_D_Y 0.098885 | Loss_G_F 3.714265\n",
            "Epoch 106 | Cost 42.9 sec\n",
            "Loss_D_X 0.028341 | Loss_D_Y 0.104557 | Loss_G_F 3.688684\n",
            "Epoch 107 | Cost 42.8 sec\n",
            "Loss_D_X 0.028516 | Loss_D_Y 0.105897 | Loss_G_F 3.650036\n",
            "Epoch 108 | Cost 42.9 sec\n",
            "Loss_D_X 0.030058 | Loss_D_Y 0.108514 | Loss_G_F 3.654779\n",
            "Epoch 109 | Cost 43.0 sec\n",
            "Loss_D_X 0.031858 | Loss_D_Y 0.096542 | Loss_G_F 3.675740\n",
            "Epoch 110 | Cost 42.9 sec\n",
            "Loss_D_X 0.023781 | Loss_D_Y 0.101874 | Loss_G_F 3.726118\n",
            "Epoch 111 | Cost 42.8 sec\n",
            "Loss_D_X 0.024095 | Loss_D_Y 0.091057 | Loss_G_F 3.664059\n",
            "Epoch 112 | Cost 42.8 sec\n",
            "Loss_D_X 0.024134 | Loss_D_Y 0.089419 | Loss_G_F 3.682230\n",
            "Epoch 113 | Cost 42.9 sec\n",
            "Loss_D_X 0.022640 | Loss_D_Y 0.097779 | Loss_G_F 3.609112\n",
            "Epoch 114 | Cost 42.9 sec\n",
            "Loss_D_X 0.028284 | Loss_D_Y 0.098254 | Loss_G_F 3.583780\n",
            "Epoch 115 | Cost 42.9 sec\n",
            "Loss_D_X 0.026451 | Loss_D_Y 0.101907 | Loss_G_F 3.551367\n",
            "Epoch 116 | Cost 42.8 sec\n",
            "Loss_D_X 0.029530 | Loss_D_Y 0.092321 | Loss_G_F 3.581783\n",
            "Epoch 117 | Cost 42.9 sec\n",
            "Loss_D_X 0.029320 | Loss_D_Y 0.091743 | Loss_G_F 3.573310\n",
            "Epoch 118 | Cost 43.0 sec\n",
            "Loss_D_X 0.025847 | Loss_D_Y 0.090801 | Loss_G_F 3.542618\n",
            "Epoch 119 | Cost 42.9 sec\n",
            "Loss_D_X 0.024464 | Loss_D_Y 0.093226 | Loss_G_F 3.508686\n",
            "Epoch 120 | Cost 44.1 sec\n",
            "Loss_D_X 0.023362 | Loss_D_Y 0.091835 | Loss_G_F 3.559121\n",
            "Epoch 121 | Cost 43.2 sec\n",
            "Loss_D_X 0.023417 | Loss_D_Y 0.091029 | Loss_G_F 3.526128\n",
            "Epoch 122 | Cost 43.0 sec\n",
            "Loss_D_X 0.022356 | Loss_D_Y 0.088477 | Loss_G_F 3.520088\n",
            "Epoch 123 | Cost 42.8 sec\n",
            "Loss_D_X 0.018839 | Loss_D_Y 0.089669 | Loss_G_F 3.576214\n",
            "Epoch 124 | Cost 42.9 sec\n",
            "Loss_D_X 0.016660 | Loss_D_Y 0.092536 | Loss_G_F 3.560488\n",
            "Epoch 125 | Cost 43.0 sec\n",
            "Loss_D_X 0.018219 | Loss_D_Y 0.086034 | Loss_G_F 3.497350\n",
            "Epoch 126 | Cost 42.8 sec\n",
            "Loss_D_X 0.021554 | Loss_D_Y 0.093704 | Loss_G_F 3.485291\n",
            "Epoch 127 | Cost 42.9 sec\n",
            "Loss_D_X 0.021011 | Loss_D_Y 0.087231 | Loss_G_F 3.431271\n",
            "Epoch 128 | Cost 42.9 sec\n",
            "Loss_D_X 0.020067 | Loss_D_Y 0.083148 | Loss_G_F 3.481663\n",
            "Epoch 129 | Cost 43.0 sec\n",
            "Loss_D_X 0.019943 | Loss_D_Y 0.080312 | Loss_G_F 3.482597\n",
            "Epoch 130 | Cost 43.0 sec\n",
            "Loss_D_X 0.018033 | Loss_D_Y 0.088106 | Loss_G_F 3.384575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U6X2VpM-Vh-"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from gen_model import Generator\n",
        "from dis_model import Discriminator\n",
        "from function import ImageDataset\n",
        "from function import ReplayBuffer\n",
        "from function import LambdaLR\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "\n",
        "transforms_tst = [transforms.ToTensor(),transforms.RandomCrop(256),transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))]\n",
        "tst_set = ImageDataset(\"./data/grumpifycat/\", transforms_=transforms_tst, mode='test')\n",
        "tst_loader = DataLoader(tst_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "\n",
        "G, F = Generator(num_blocks=9), Generator(num_blocks=9)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "G.to(device)\n",
        "F.to(device)\n",
        "G.load_state_dict(torch.load('output/G.pth'))\n",
        "F.load_state_dict(torch.load('output/F.pth'))\n",
        "G.eval\n",
        "F.eval\n",
        "\n",
        "'''\n",
        "if not os.path.exists('output/X'):\n",
        "    os.makedirs('output/X')\n",
        "if not os.path.exists('output/Y'):\n",
        "    os.makedirs('output/Y')\n",
        "'''\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor\n",
        "#Tensor = torch.Tensor\n",
        "\n",
        "for idx, data in enumerate(tst_loader):\n",
        "\n",
        "    X = Variable(Tensor(batch_size, 3, 256, 256).copy_(data['X']))\n",
        "    Y = Variable(Tensor(batch_size, 3, 256, 256).copy_(data['Y']))\n",
        "    G_X, F_Y = G(X), F(Y)\n",
        "    F_G_X, G_F_Y = F(G_X), G(F_Y)\n",
        "\n",
        "    origin_X = 0.5 * (X.data + 1.0)\n",
        "    origin_Y = 0.5 * (Y.data + 1.0)\n",
        "    fake_X= 0.5 * (G_X.data + 1.0)\n",
        "    fake_Y = 0.5 * (F_Y.data + 1.0)\n",
        "    cycle_X = 0.5 * (F_G_X.data + 1.0)\n",
        "    cycle_Y = 0.5 * (G_F_Y.data + 1.0)\n",
        "\n",
        "    save_image(origin_X, 'output/X/%04dorigin.png' % (idx+1))\n",
        "    save_image(origin_Y, 'output/Y/%04dorigin.png' % (idx+1))\n",
        "    save_image(fake_X, 'output/X/%04dfake.png' % (idx+1))\n",
        "    save_image(fake_Y, 'output/Y/%04dfake.png' % (idx+1))\n",
        "    save_image(cycle_X, 'output/X/%04dcycle.png' % (idx+1))\n",
        "    save_image(cycle_Y, 'output/Y/%04dcycle.png' % (idx+1))"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}